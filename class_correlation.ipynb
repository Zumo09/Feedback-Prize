{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "class_correlation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import csv\n",
        "from math import log10"
      ],
      "metadata": {
        "id": "oiNLHisJl4LN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "WEIbKiFol-jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "not_word = [',', ':', '.', '?', '!', ')', '(', ';', '/', '-', '\"', '\\'', '_']\n",
        "\n",
        "def cleaning(text:str) -> str:\n",
        "\n",
        "  text = text.lower()\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  for char in not_word:\n",
        "    text = text.replace(char, ' ')\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def count_words(l : list) -> dict:\n",
        "  words = l.split()\n",
        "\n",
        "  return Counter(words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-mFBms8wmkP",
        "outputId": "7b7f5b33-59aa-4359-9264-1f972028e9c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_grouped = data.groupby(by='discourse_type')"
      ],
      "metadata": {
        "id": "T_155VoinEtd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, gp in class_grouped:\n",
        "  group = class_grouped.get_group(key)['discourse_text']\n",
        "  l=''\n",
        "  for i in range(len(group)):\n",
        "    text = group.iloc[i]\n",
        "    text = cleaning(text)\n",
        "    l = l + text\n",
        "  \n",
        "  words_count = count_words(l)\n",
        "\n",
        "  f = open(key + \".csv\", \"w\")\n",
        "  writer = csv.writer(f)\n",
        "  for key, value in words_count.items():\n",
        "    writer.writerow([key, value])\n",
        "\n",
        "  f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ano65T_C43Dq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reader = csv.reader(open('Claim.csv', 'r'))\n",
        "dict_claim = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_claim[k] = v\n",
        "\n",
        "reader = csv.reader(open('Concluding Statement.csv', 'r'))\n",
        "dict_concluding = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_concluding[k] = v\n",
        "\n",
        "reader = csv.reader(open('Counterclaim.csv', 'r'))\n",
        "dict_counterclaim = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_counterclaim[k] = v\n",
        "\n",
        "\n",
        "reader = csv.reader(open('Evidence.csv', 'r'))\n",
        "dict_evidence = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_evidence[k] = v\n",
        "\n",
        "reader = csv.reader(open('Lead.csv', 'r'))\n",
        "dict_lead = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_lead[k] = v\n",
        "\n",
        "reader = csv.reader(open('Position.csv', 'r'))\n",
        "dict_position = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_position[k] = v\n",
        "\n",
        "reader = csv.reader(open('Rebuttal.csv', 'r'))\n",
        "dict_rebuttal = {}\n",
        "for row in reader:\n",
        "   k, v = row\n",
        "   dict_rebuttal[k] = v"
      ],
      "metadata": {
        "id": "NgY7t2X1EwA7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def correlation(word : str) -> np.array:\n",
        "\n",
        "  n_claim = int(dict_claim[word]) if word in dict_claim else 0\n",
        "  n_concluding = int(dict_concluding[word]) if word in dict_concluding else 0\n",
        "  n_counterclaim = int(dict_counterclaim[word]) if word in dict_counterclaim else 0\n",
        "  n_evidence = int(dict_evidence[word]) if word in dict_evidence else 0\n",
        "  n_lead = int(dict_lead[word]) if word in dict_lead else 0\n",
        "  n_position = int(dict_position[word]) if word in dict_position else 0\n",
        "  n_rebuttal = int(dict_rebuttal[word]) if word in dict_rebuttal else 0\n",
        "\n",
        "\n",
        "  total = n_claim + n_concluding + n_counterclaim + n_evidence + n_lead + n_position + n_rebuttal\n",
        "  idf = log10(144293/total)\n",
        "  return np.array((n_claim/len(dict_claim), n_concluding/len(dict_concluding), n_counterclaim/len(dict_counterclaim), \\\n",
        "                   n_evidence/(len(dict_evidence)), n_lead/len(dict_lead), n_position/len(dict_position), n_rebuttal/len(dict_rebuttal)))*idf\n"
      ],
      "metadata": {
        "id": "U0wpGfKsDtaG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = set()\n",
        "\n",
        "for w in dict_claim:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_concluding:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_counterclaim:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_evidence:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_lead:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_position:\n",
        "  word_list.add(w)\n",
        "\n",
        "for w in dict_rebuttal:\n",
        "  word_list.add(w)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1kbvkYeGgoX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.zeros((len(word_list), 7))\n",
        "i = 0\n",
        "for w in word_list:\n",
        "  matrix[i, :] = correlation(w)\n",
        "  i += 1\n",
        "  "
      ],
      "metadata": {
        "id": "ANHD1yDwHPOi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_matrix = pd.DataFrame(matrix, columns =['Claim', 'Concluding Statement', 'Counterclaim', 'Evidence', 'Lead', 'Position', 'Rebuttal'])\n",
        "words = pd.DataFrame(word_list, columns=['Words'])\n",
        "p = pd.concat([words, weights_matrix], axis=1)"
      ],
      "metadata": {
        "id": "Mg6h2v9TmeqQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.to_csv('weights_matrix.csv')"
      ],
      "metadata": {
        "id": "cFvDVxHpnyQb"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}