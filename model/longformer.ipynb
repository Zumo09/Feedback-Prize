{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "longformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO87dCDFvR29A7miP5KrDv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zumo09/Feedback-Prize/blob/main/model/longformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k178G_XkD0G"
      },
      "outputs": [],
      "source": [
        "from transformers import LongformerTokenizerFast, LongformerForTokenClassification, LongformerPreTrainedModel, LongformerModel\n",
        "from transformers import LongformerClassificationHead, LongformerConfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a Longformer for multilabel classification class\n",
        "class LongformerForMultiLabelSequenceClassification(LongformerPreTrainedModel):\n",
        "    \"\"\"\n",
        "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
        "    This instance takes the pooled output of the LongFormer based model and passes it through a classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities for all the labels that we feed into the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LongformerForMultiLabelSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.longformer = LongformerModel(config)\n",
        "        self.classifier = LongformerClassificationHead(config)\n",
        "        self.init_weights()\n",
        "        \n",
        "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
        "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
        "                labels=None):\n",
        "        \n",
        "        # create global attention on sequence, and a global attention token on the `s` token\n",
        "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
        "        # on the LongformerForSequenceClassification class\n",
        "        if global_attention_mask is None:\n",
        "            global_attention_mask = torch.zeros_like(input_ids)\n",
        "            global_attention_mask[:, 0] = 1\n",
        "        \n",
        "        # pass arguments to longformer model\n",
        "        outputs = self.longformer(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            global_attention_mask = global_attention_mask,\n",
        "            token_type_ids = token_type_ids,\n",
        "            position_ids = position_ids)\n",
        "        \n",
        "        # if specified the model can return a dict where each key corresponds to the output of a\n",
        "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
        "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
        "        sequence_output = outputs['last_hidden_state']\n",
        "        \n",
        "        # pass the hidden states through the classifier to obtain thee logits\n",
        "        logits = self.classifier(sequence_output)\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "        if labels is not None:\n",
        "            #loss_fct = BCEWithLogitsLoss()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            labels = labels.float()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
        "                            labels.view(-1, self.num_labels))\n",
        "            #outputs = (loss,) + outputs\n",
        "            outputs = (loss,) + outputs\n",
        "        \n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "5Y_DFuY-pjWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a Longformer configuration\n",
        "configuration = LongformerConfig()\n",
        "\n",
        "# Initializing a model from the configuration\n",
        "model = LongformerModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config\n",
        "\n",
        "congiguration.num_labels = 15"
      ],
      "metadata": {
        "id": "GM76EQ8h1rKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}