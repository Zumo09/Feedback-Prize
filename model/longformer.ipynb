{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "longformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnBE0diRh0mH5BcLV2cMQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zumo09/Feedback-Prize/blob/main/model/longformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install positional_encodings"
      ],
      "metadata": {
        "id": "fVUuD-_Hisez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7k178G_XkD0G"
      },
      "outputs": [],
      "source": [
        "from transformers import LongformerTokenizerFast, LongformerConfig, LongformerModel, LEDConfig, LEDModel\n",
        "#from torch.nn import Linear, Sequential, ReLU, Module, ModuleList, Conv1d\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from positional_encodings import PositionalEncodingPermute1D, Summer\n",
        "from typing import Optional\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model  = LEDModel.from_pretrained('allenai/led-base-16384')"
      ],
      "metadata": {
        "id": "aZ5rQlnG9hMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LongformerModel"
      ],
      "metadata": {
        "id": "4gbD9RCCMBOm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model."
      ],
      "metadata": {
        "id": "W9o0wyzugPMa",
        "outputId": "5ebe74e0-a172-4c9e-ace1-81dd939fc4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-049bbf2d8f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'LongformerModel' has no attribute 'out_channels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = LEDConfig.from_pretrained('allenai/led-base-16384')\n",
        "print(config)"
      ],
      "metadata": {
        "id": "iXqopuSFGXYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, model, norm_encoder = False, norm_decoder = False):#, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = model.encoder.layers # It keep just the layers related to the encoder parts and remove the embedding layers.\n",
        "        self.decoder = model.decoder.layers # Same as before. We have to decide if is good to keep or remove embeddings layers\n",
        "\n",
        "        self.norm_encoder = model.encoder.layernorm_embedding if norm_encoder else None # To decide if we want to keep layernorm_embedding\n",
        "        self.norm_decoder = model.decoder.layernorm_embedding if norm_decoder else None\n",
        "        \"\"\"\n",
        "        self.prediction_box = MLP(output_dim=2, num_layers=3) # 2 dimensions beacuse it predict center and length\n",
        "        self.prediction_class = nn.Linear(out_features = num_classes + 1)\n",
        "        \"\"\"\n",
        "    def forward(self, src,\n",
        "                d : int, \n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                reduce_channels = True, \n",
        "                #pos: Optional[Tensor] = None\n",
        "                ):\n",
        "      \n",
        "        output = src\n",
        "\n",
        "        if reduce_channels:\n",
        "          '''\n",
        "          In the paper they reduce the size of the embeddings before the encoding,\n",
        "          but they start with 2048 channels, our embeddings have 606 channels.\n",
        "          We have to decide if reducing the number of channels is a good idea.\n",
        "          '''\n",
        "          output = self.conv(in_channels = src.size()[1], out_channels=d, kernel=1)\n",
        "\n",
        "        for layer in self.encoder:\n",
        "            '''\n",
        "            To adjust depending on the value we want to use\n",
        "\n",
        "            '''\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "            \n",
        "        if self.norm_encoder is not None:\n",
        "          \n",
        "            output = self.norm_encoder(output)\n",
        "            \n",
        "\n",
        "        for layer in self.decoder:\n",
        "\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm_decoder is not None:\n",
        "          \n",
        "            output = self.norm_decoder(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "yTMgok7UKWpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, embedder, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1) # +1 for background class\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 2, 3) # 2 dimensions because it predicts center and length\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv1d(embedder.num_channels, hidden_dim, kernel_size=1)\n",
        "        #self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "WYuXG3N05teB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder(nn.Module):\n",
        "\n",
        "    '''\n",
        "    This class takes care of generating the embeddings for each text using a pretrained\n",
        "    Longformer, then the number of channels is reduced using a 1x1 convolutional layer.\n",
        "    The output are the embeddings plus the positional encodings.\n",
        "    Input:\n",
        "      embedder  : The model used for generating the embeddings\n",
        "      tokenizer : The tokenizer required by the model\n",
        "      text      : The input text to embed\n",
        "      d         : The number of output channel. (the name follows the coventon of the paper)\n",
        "    Output:\n",
        "      posit_encoder : embeddings + positional encodings\n",
        "    '''\n",
        "  \n",
        "    def __init__(self, embedder, tokenizer):\n",
        "      super(Embedder, self).__init__()\n",
        "      #self.max_length = max_length\n",
        "      self.embedder = embedder\n",
        "      self.tokenizer = tokenizer\n",
        "\n",
        "  \n",
        "    def forward(self, text : str):\n",
        "\n",
        "      input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "      output = self.embedder(input_ids)['last_hidden_state']\n",
        "      #reduced_output = Conv1d(in_channels = output.size()[1], out_channels = d, kernel = 1)\n",
        "      posit_encoder = Summer(PositionalEncodingPermute1D(output.size()[1])) # Numbers of channels\n",
        "\n",
        "\n",
        "      return posit_encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "lf3DbcWK57aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = nn.ReLU(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YrWFBcx9r0L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For summarization\n"
      ],
      "metadata": {
        "id": "Lcp2FPTgOr6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, EncoderDecoderModel\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/longformer2roberta-cnn_dailymail-fp16\")\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\") \n",
        "\n",
        "# Specify the article\n",
        "article = \"\"\"Germany (German: Deutschland, German pronunciation: [ˈdɔʏtʃlant]), officially the Federal Republic of Germany,[e] is a country at the intersection of Central and Western Europe. It is situated between the Baltic and North seas to the north, and the Alps to the south; covering an area of 357,022 square kilometres (137,847 sq mi), with a population of over 83 million within its 16 constituent states. It borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium, and the Netherlands to the west. Germany is the second-most populous country in Europe after Russia, as well as the most populous member state of the European Union. Its capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before AD 100. In the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation. Following the Napoleonic Wars and the dissolution of the Holy Roman Empire in 1806, the German Confederation was formed in 1815. In 1871, Germany became a nation-state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the semi-presidential Weimar Republic. The Nazi seizure of power in 1933 led to the establishment of a dictatorship, World War II, and the Holocaust. After the end of World War II in Europe and a period of Allied occupation, Germany was divided into the Federal Republic of Germany, generally known as West Germany, and the German Democratic Republic, East Germany. The Federal Republic of Germany was a founding member of the European Economic Community and the European Union, while the German Democratic Republic was a communist Eastern Bloc state and member of the Warsaw Pact. After the fall of communism, German reunification saw the former East German states join the Federal Republic of Germany on 3 October 1990—becoming a federal parliamentary republic led by a chancellor.Germany is a great power with a strong economy; it has the largest economy in Europe, the world's fourth-largest economy by nominal GDP, and the fifth-largest by PPP. As a global leader in several industrial, scientific and technological sectors, it is both the world's third-largest exporter and importer of goods. As a developed country, which ranks very high on the Human Development Index, it offers social security and a universal health care system, environmental protections, and a tuition-free university education. Germany is also a member of the United Nations, NATO, the G7, the G20, and the OECD. It also has the fourth-greatest number of UNESCO World Heritage Sites.\"\"\"\n",
        "\n",
        "# Tokenize and summarize\n",
        "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
        "output_ids = model.generate(input_ids)\n",
        "\n",
        "# Get the summary from the output tokens\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "O5KRoUwcNt39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('F4A4E65ADD95.txt') as f:\n",
        "  article = f.read()\n",
        "\n",
        "#article = \"\"\"Germany (German: Deutschland, German pronunciation: [ˈdɔʏtʃlant]), officially the Federal Republic of Germany,[e] is a country at the intersection of Central and Western Europe. It is situated between the Baltic and North seas to the north, and the Alps to the south; covering an area of 357,022 square kilometres (137,847 sq mi), with a population of over 83 million within its 16 constituent states. It borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium, and the Netherlands to the west. Germany is the second-most populous country in Europe after Russia, as well as the most populous member state of the European Union. Its capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before AD 100. In the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation. Following the Napoleonic Wars and the dissolution of the Holy Roman Empire in 1806, the German Confederation was formed in 1815. In 1871, Germany became a nation-state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the semi-presidential Weimar Republic. The Nazi seizure of power in 1933 led to the establishment of a dictatorship, World War II, and the Holocaust. After the end of World War II in Europe and a period of Allied occupation, Germany was divided into the Federal Republic of Germany, generally known as West Germany, and the German Democratic Republic, East Germany. The Federal Republic of Germany was a founding member of the European Economic Community and the European Union, while the German Democratic Republic was a communist Eastern Bloc state and member of the Warsaw Pact. After the fall of communism, German reunification saw the former East German states join the Federal Republic of Germany on 3 October 1990—becoming a federal parliamentary republic led by a chancellor.Germany is a great power with a strong economy; it has the largest economy in Europe, the world's fourth-largest economy by nominal GDP, and the fifth-largest by PPP. As a global leader in several industrial, scientific and technological sectors, it is both the world's third-largest exporter and importer of goods. As a developed country, which ranks very high on the Human Development Index, it offers social security and a universal health care system, environmental protections, and a tuition-free university education. Germany is also a member of the United Nations, NATO, the G7, the G20, and the OECD. It also has the fourth-greatest number of UNESCO World Heritage Sites.\"\"\"\n",
        "\n",
        "# Tokenize and summarize\n",
        "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
        "output_ids = model.generate(input_ids)\n",
        "\n",
        "# Get the summary from the output tokens\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3OtML11OTWX",
        "outputId": "8f817432-aed4-4b1d-d056-913e4ed4d3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confiding in multiple people when looking for advice will give you a more reliable response.\n",
            "Asking multiple people about their experiences will give them a more detailed response. and give you more detailed responses.\n",
            "Ask multiple different people for their advice.\n",
            "People around you may have prior experience or opinions that can help you decide on a course of action.\n"
          ]
        }
      ]
    }
  ]
}