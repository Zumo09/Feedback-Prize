{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "longformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsNg1EGDJQ6s7NrO/GE8Ew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zumo09/Feedback-Prize/blob/main/model/longformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fVUuD-_Hisez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7k178G_XkD0G"
      },
      "outputs": [],
      "source": [
        "from transformers import LongformerTokenizerFast, LongformerConfig, LongformerModel, LEDConfig, LEDModel\n",
        "from torch.nn import Linear, Sequential, ReLU\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.state_dict():\n",
        "  print(param)"
      ],
      "metadata": {
        "id": "3zioXBCEUAHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LEDConfig.from_pretrained('allenai/led-base-16384', output_hidden_states=True)\n",
        "model = LEDModel(config)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YKXQSQwHR-Ig",
        "outputId": "a7dcde9a-da9a-468f-c582-590cc78ecd90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LEDModel(\n",
              "  (shared): Embedding(50265, 768, padding_idx=1)\n",
              "  (encoder): LEDEncoder(\n",
              "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "    (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n",
              "    (layers): ModuleList(\n",
              "      (0): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): LEDEncoderLayer(\n",
              "        (self_attn): LEDEncoderAttention(\n",
              "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): LEDDecoder(\n",
              "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "    (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
              "    (layers): ModuleList(\n",
              "      (0): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): LEDDecoderLayer(\n",
              "        (self_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder_attn): LEDDecoderAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a Longformer for multilabel classification class\n",
        "class LED(LEDModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LED, self).__init__(config)\n",
        "        self.num_labels = config.num_labels + 1 # For no object class\n",
        "        self.longformer = LEDModel(config)\n",
        "        self.classifier = Linear(in_features = config['hidden_size'], out_features=self.num_labels) # it produces as output the class of the sequence\n",
        "        self.bb_estimator = Sequential(Linear(in_features=config['hidden_size'], out_features=sequence_length),\n",
        "                                       ReLU(),\n",
        "                                       Linear(in_features=sequence_length, out_features=d), # to define d\n",
        "                                       ReLU(),\n",
        "                                       Linear(in_features=d, out_features=2))# It produces as output the center and the lenght of the segment to classify\n",
        "        self.init_weights()\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
        "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
        "                labels=None):\n",
        "        \n",
        "        # create global attention on sequence, and a global attention token on the `s` token\n",
        "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
        "        # on the LongformerForSequenceClassification class\n",
        "        if global_attention_mask is None:\n",
        "            global_attention_mask = torch.zeros_like(input_ids)\n",
        "            global_attention_mask[:] = 1\n",
        "        \n",
        "        # pass arguments to longformer model\n",
        "        outputs = self.longformer(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            global_attention_mask = global_attention_mask,\n",
        "            token_type_ids = token_type_ids,\n",
        "            position_ids = position_ids)\n",
        "        \n",
        "        # if specified the model can return a dict where each key corresponds to the output of a\n",
        "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
        "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
        "        sequence_output = outputs['last_hidden_state']\n",
        "        \n",
        "        # pass the hidden states through the classifier to obtain thee logits\n",
        "        #logits = self.classifier(sequence_output)\n",
        "        bb_center, bb_lenght = self.bb_estimator(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        return logits, bb\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "DAvMqYTZ0l_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a Longformer for multilabel classification class\n",
        "class Longformer(LongformerModel):\n",
        "    \"\"\"\n",
        "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
        "    This instance takes the pooled output of the LongFormer based model and passes it through a classification head. We replace \n",
        "    the traditional Cross Entropy loss with a BCE loss that generate probabilities for all the labels that we feed into the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(Longformer, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.longformer = LongformerModel(config)\n",
        "        self.classifier = Linear(in_features = config['hidden_size'], out_features=self.num_labels) # it produces as output the class of the sequence\n",
        "        self.bb_estimator = Sequential(Linear(in_features=config['hidden_size'], out_features=sequence_length),\n",
        "                                       Linear(in_features=sequence_length, out_features=2))# It produces as output the center and the lenght of the segment to classify\n",
        "        self.init_weights()\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
        "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
        "                labels=None):\n",
        "        \n",
        "        # create global attention on sequence, and a global attention token on the `s` token\n",
        "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
        "        # on the LongformerForSequenceClassification class\n",
        "        if global_attention_mask is None:\n",
        "            global_attention_mask = torch.zeros_like(input_ids)\n",
        "            global_attention_mask[:] = 1\n",
        "        \n",
        "        # pass arguments to longformer model\n",
        "        outputs = self.longformer(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            global_attention_mask = global_attention_mask,\n",
        "            token_type_ids = token_type_ids,\n",
        "            position_ids = position_ids)\n",
        "        \n",
        "        # if specified the model can return a dict where each key corresponds to the output of a\n",
        "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
        "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
        "        sequence_output = outputs['last_hidden_state']\n",
        "        \n",
        "        # pass the hidden states through the classifier to obtain thee logits\n",
        "        #logits = self.classifier(sequence_output)\n",
        "        bb_center, bb_lenght = self.bb_estimator(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        return logits, bb\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "FdBepDrHeu1q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_ids = torch.Tensor(token_text['input_ids'])\n",
        "input_ids = torch.tensor(tokenizer.encode(text1)).unsqueeze(0)\n",
        "out = longf(input_ids)"
      ],
      "metadata": {
        "id": "kDYj3E4AmpoK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText for embeddings words and SIF for documents embeddings\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import fasttext\n",
        "\n",
        "model = fasttext.train_unsupervised('FC8DD899209B.txt', model='skipgram')\n",
        "def get_weighted_average(We, x, w):\n",
        "    \"\"\"\n",
        "    Compute the weighted average vectors\n",
        "    :param We: We[i,:] is the vector for word i\n",
        "    :param x: x[i, :] are the indices of the words in sentence i\n",
        "    :param w: w[i, :] are the weights for the words in sentence i\n",
        "    :return: emb[i, :] are the weighted average vector for sentence i\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "    emb = np.zeros((n_samples, We.shape[1]))\n",
        "    for i in xrange(n_samples):\n",
        "        emb[i,:] = w[i,:].dot(We[x[i,:],:]) / np.count_nonzero(w[i,:])\n",
        "    return emb\n",
        "\n",
        "def compute_pc(X,npc=1):\n",
        "    \"\"\"\n",
        "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
        "    :param X: X[i,:] is a data point\n",
        "    :param npc: number of principal components to remove\n",
        "    :return: component_[i,:] is the i-th pc\n",
        "    \"\"\"\n",
        "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
        "    svd.fit(X)\n",
        "    return svd.components_\n",
        "\n",
        "def remove_pc(X, npc=1):\n",
        "    \"\"\"\n",
        "    Remove the projection on the principal components\n",
        "    :param X: X[i,:] is a data point\n",
        "    :param npc: number of principal components to remove\n",
        "    :return: XX[i, :] is the data point after removing its projection\n",
        "    \"\"\"\n",
        "    pc = compute_pc(X, npc)\n",
        "    if npc==1:\n",
        "        XX = X - X.dot(pc.transpose()) * pc\n",
        "    else:\n",
        "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
        "    return XX\n",
        "\n",
        "\n",
        "def SIF_embedding(We, x, w, params):\n",
        "    \"\"\"\n",
        "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
        "    :param We: We[i,:] is the vector for word i\n",
        "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
        "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
        "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
        "    :return: emb, emb[i, :] is the embedding for sentence i\n",
        "    \"\"\"\n",
        "    emb = get_weighted_average(We, x, w)\n",
        "    if  params.rmpc > 0:\n",
        "        emb = remove_pc(emb, params.rmpc)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "rvOsB1av_W8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For summarization\n"
      ],
      "metadata": {
        "id": "Lcp2FPTgOr6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, EncoderDecoderModel\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/longformer2roberta-cnn_dailymail-fp16\")\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\") \n",
        "\n",
        "# Specify the article\n",
        "article = \"\"\"Germany (German: Deutschland, German pronunciation: [ˈdɔʏtʃlant]), officially the Federal Republic of Germany,[e] is a country at the intersection of Central and Western Europe. It is situated between the Baltic and North seas to the north, and the Alps to the south; covering an area of 357,022 square kilometres (137,847 sq mi), with a population of over 83 million within its 16 constituent states. It borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium, and the Netherlands to the west. Germany is the second-most populous country in Europe after Russia, as well as the most populous member state of the European Union. Its capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before AD 100. In the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation. Following the Napoleonic Wars and the dissolution of the Holy Roman Empire in 1806, the German Confederation was formed in 1815. In 1871, Germany became a nation-state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the semi-presidential Weimar Republic. The Nazi seizure of power in 1933 led to the establishment of a dictatorship, World War II, and the Holocaust. After the end of World War II in Europe and a period of Allied occupation, Germany was divided into the Federal Republic of Germany, generally known as West Germany, and the German Democratic Republic, East Germany. The Federal Republic of Germany was a founding member of the European Economic Community and the European Union, while the German Democratic Republic was a communist Eastern Bloc state and member of the Warsaw Pact. After the fall of communism, German reunification saw the former East German states join the Federal Republic of Germany on 3 October 1990—becoming a federal parliamentary republic led by a chancellor.Germany is a great power with a strong economy; it has the largest economy in Europe, the world's fourth-largest economy by nominal GDP, and the fifth-largest by PPP. As a global leader in several industrial, scientific and technological sectors, it is both the world's third-largest exporter and importer of goods. As a developed country, which ranks very high on the Human Development Index, it offers social security and a universal health care system, environmental protections, and a tuition-free university education. Germany is also a member of the United Nations, NATO, the G7, the G20, and the OECD. It also has the fourth-greatest number of UNESCO World Heritage Sites.\"\"\"\n",
        "\n",
        "# Tokenize and summarize\n",
        "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
        "output_ids = model.generate(input_ids)\n",
        "\n",
        "# Get the summary from the output tokens\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "O5KRoUwcNt39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('F4A4E65ADD95.txt') as f:\n",
        "  article = f.read()\n",
        "\n",
        "#article = \"\"\"Germany (German: Deutschland, German pronunciation: [ˈdɔʏtʃlant]), officially the Federal Republic of Germany,[e] is a country at the intersection of Central and Western Europe. It is situated between the Baltic and North seas to the north, and the Alps to the south; covering an area of 357,022 square kilometres (137,847 sq mi), with a population of over 83 million within its 16 constituent states. It borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium, and the Netherlands to the west. Germany is the second-most populous country in Europe after Russia, as well as the most populous member state of the European Union. Its capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before AD 100. In the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation. Following the Napoleonic Wars and the dissolution of the Holy Roman Empire in 1806, the German Confederation was formed in 1815. In 1871, Germany became a nation-state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the semi-presidential Weimar Republic. The Nazi seizure of power in 1933 led to the establishment of a dictatorship, World War II, and the Holocaust. After the end of World War II in Europe and a period of Allied occupation, Germany was divided into the Federal Republic of Germany, generally known as West Germany, and the German Democratic Republic, East Germany. The Federal Republic of Germany was a founding member of the European Economic Community and the European Union, while the German Democratic Republic was a communist Eastern Bloc state and member of the Warsaw Pact. After the fall of communism, German reunification saw the former East German states join the Federal Republic of Germany on 3 October 1990—becoming a federal parliamentary republic led by a chancellor.Germany is a great power with a strong economy; it has the largest economy in Europe, the world's fourth-largest economy by nominal GDP, and the fifth-largest by PPP. As a global leader in several industrial, scientific and technological sectors, it is both the world's third-largest exporter and importer of goods. As a developed country, which ranks very high on the Human Development Index, it offers social security and a universal health care system, environmental protections, and a tuition-free university education. Germany is also a member of the United Nations, NATO, the G7, the G20, and the OECD. It also has the fourth-greatest number of UNESCO World Heritage Sites.\"\"\"\n",
        "\n",
        "# Tokenize and summarize\n",
        "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
        "output_ids = model.generate(input_ids)\n",
        "\n",
        "# Get the summary from the output tokens\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3OtML11OTWX",
        "outputId": "8f817432-aed4-4b1d-d056-913e4ed4d3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confiding in multiple people when looking for advice will give you a more reliable response.\n",
            "Asking multiple people about their experiences will give them a more detailed response. and give you more detailed responses.\n",
            "Ask multiple different people for their advice.\n",
            "People around you may have prior experience or opinions that can help you decide on a course of action.\n"
          ]
        }
      ]
    }
  ]
}